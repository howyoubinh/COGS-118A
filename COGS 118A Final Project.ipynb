{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118A Final Project\n",
    "Binh Nguyen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import string\n",
    "import csv\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets acquired from UCI Machine Learning Repository:\n",
    "https://archive.ics.uci.edu/ml/datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1. LETTER Dataset\n",
    "https://archive.ics.uci.edu/ml/datasets/letter+recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size = 20000x17\n",
    "# N = 20000\n",
    "# D = 16 features\n",
    "data_letter = np.genfromtxt('letter-recognition.data', delimiter = ',', dtype=None, encoding=None)\n",
    "n_letter = np.size(data_letter)\n",
    "d_letter = 16\n",
    "X_letter = np.zeros((n_letter,d_letter))\n",
    "\n",
    "# Create X Matrix from numpy.void type\n",
    "for i in range(n_letter):\n",
    "    for j in range(d_letter):\n",
    "        X_letter[i][j] = data_letter[i][j+1]\n",
    "        \n",
    "# Create y vector with labels for each letter\n",
    "y_letter = []\n",
    "for i in range(n_letter):\n",
    "    y_letter.append(data_letter[i][0])\n",
    "\n",
    "# Convert letters to positive and negative values\n",
    "\n",
    "# A-M as positive (+) = 1\n",
    "positive = list(string.ascii_uppercase)[0:13]\n",
    "\n",
    "# N-Z as negative (-) = 0\n",
    "negative = list(string.ascii_uppercase)[13:26]\n",
    "\n",
    "y_letter = np.asarray(y_letter)\n",
    "\n",
    "for i in range(len(y_letter)):\n",
    "    for j in range(len(positive)):\n",
    "        if y_letter[i] == positive[j]:\n",
    "            y_letter[i] = 1\n",
    "        elif y_letter[i] == negative[j]:\n",
    "            y_letter[i] = 0\n",
    "\n",
    "y_letter = y_letter.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Indian Pines dataset\n",
    "http://www.ehu.eus/ccwintco/index.php?title=Hyperspectral_Remote_Sensing_Scenes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 145 x 145 x 200 multi-dimensional array\n",
    "# 145 x 145 pixel images\n",
    "# 200 samples\n",
    "# class 11: Soybean-minmill as positive, rest as negative\n",
    "mat = sio.loadmat('Indian_pines_corrected.mat')\n",
    "values_ip = mat.values()\n",
    "X_ip = values_ip[1][0:][0:]\n",
    "\n",
    "# Load ground truth (y labels)\n",
    "mat2 = sio.loadmat('Indian_pines_gt.mat')\n",
    "values2_ip = mat2.values()\n",
    "y_ip = values2_ip[2][0:][0:]\n",
    "\n",
    "# Convert class 11 (Soybean-mintill) labels in ground truth as positive\n",
    "y_ip[y_ip[0:] == 11] = 1\n",
    "y_ip[y_ip[0:] != 1] = 0\n",
    "\n",
    "# Convert 3-D array to 2-D array\n",
    "X_ip = X_ip.transpose(2,0,1).reshape(21025, -1)\n",
    "y_ip = y_ip.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Yeast dataset\n",
    "https://archive.ics.uci.edu/ml/datasets/Yeast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 1484 datapoints\n",
    "# d = 8 features\n",
    "data_yeast = np.genfromtxt('yeast.data.csv', delimiter = ',', dtype=None, encoding=None)\n",
    "n_yeast = np.size(data_yeast)\n",
    "d_yeast = 8\n",
    "X_yeast = np.zeros((n_yeast,d_yeast))\n",
    "\n",
    "# Create X Matrix from numpy.void type\n",
    "for i in range(n_yeast):\n",
    "    for j in range(d_yeast):\n",
    "        X_yeast[i][j] = data_yeast[i][j+1]\n",
    "        \n",
    "# Create y vector with labels for each letter\n",
    "y_yeast = []\n",
    "for i in range(n_yeast):\n",
    "    y_yeast.append(data_yeast[i][d_yeast+1])\n",
    "    \n",
    "not_nuclear = ['CYT','MIT','ME3','ME2','ME1','EXC','VAC','POX','ERL']\n",
    "\n",
    "y_yeast = np.asarray(y_yeast)\n",
    "\n",
    "for i in range(len(y_yeast)):\n",
    "    for j in range(len(not_nuclear)):\n",
    "        if y_yeast[i] == 'NUC':\n",
    "            y_yeast[i] = 1\n",
    "        elif y_yeast[i] == not_nuclear[j]:\n",
    "            y_yeast[i] = 0\n",
    "\n",
    "y_yeast = y_yeast.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CV(X, y, folds, test_size, model, values):\n",
    "    ''' \n",
    "    K-Fold Cross Validation:\n",
    "    values = parameters to test in 1-dimensional array\n",
    "        -e.g. values = [1, 10, 100, 100]\n",
    "    X = training data\n",
    "        -e.g X = X_letter\n",
    "    folds = k number of folds\n",
    "        -e.g. folds = 5 % for 5-fold CV\n",
    "    test_size = percent of training data to be tested\n",
    "        -e.g. test_size = 0.2 % 20% of training data as validation set\n",
    "    model = scikit.learn classifier function\n",
    "        -e.g. model= BAG_DT(values, max_samp, max_feat)\n",
    "            % For bagging decision tree\n",
    "    '''\n",
    "    n = len(X)\n",
    "    kf = KFold(n_splits = folds)\n",
    "    splits = kf.get_n_splits()\n",
    "    optimal_p = values[0]\n",
    "    count = 0\n",
    "    avg_acc = np.zeros(len(values))\n",
    "    avg_train_acc = np.zeros(len(values))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size)\n",
    "    \n",
    "    # Iterate over K-Fold cross validation\n",
    "    for i in values:\n",
    "        fold = 0\n",
    "        score_test = np.zeros(splits)\n",
    "        score_train = np.zeros(splits)\n",
    "        print \"\\np =\", i\n",
    "        \n",
    "    # Test Validation error for each fold\n",
    "        for train_idx, test_idx in kf.split(X_train):\n",
    "            fold += 1\n",
    "            X_train_set, X_test_set = X_train[train_idx], X_train[test_idx]\n",
    "            y_train_set, y_test_set = y_train[train_idx], y_train[test_idx]\n",
    "            \n",
    "            clf = model\n",
    "            clf = clf.fit(X_train_set, y_train_set)\n",
    "            \n",
    "            # Save accuracy to vector\n",
    "            score_train[fold - 1] = clf.score(X_train_set, y_train_set)\n",
    "            score_test[fold - 1] = clf.score(X_test_set, y_test_set)\n",
    "            \n",
    "        avg_train_acc[count] = np.average(score_train)\n",
    "        avg_acc[count] = np.average(score_test)\n",
    "        \n",
    "        print \"Avg training accuracy: %f\" % (avg_train_acc[count])\n",
    "        print \"Avg validation accuracy %f\" % (avg_acc[count])\n",
    "        count += 1\n",
    "    \n",
    "    index_optimal_p = avg_acc.tolist().index(max(avg_acc))\n",
    "    optimal_p = values[index_optimal_p]\n",
    "    \n",
    "    print \"\\nOptimal p:\", optimal_p\n",
    "    print \"Best validation accuracy:\", np.amax(avg_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BAG_DT(i, max_samp, max_feat):\n",
    "    clf = BaggingClassifier(\n",
    "        DecisionTreeClassifier(max_depth = i),\n",
    "            max_samples = max_samp,\n",
    "            max_features = max_feat)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "p = 1\n",
      "Avg training accuracy: 0.985750\n",
      "Avg validation accuracy 0.924250\n",
      "\n",
      "p = 10\n",
      "Avg training accuracy: 0.982063\n",
      "Avg validation accuracy 0.912937\n",
      "\n",
      "p = 100\n",
      "Avg training accuracy: 0.983437\n",
      "Avg validation accuracy 0.913188\n",
      "\n",
      "p = 1000\n",
      "Avg training accuracy: 0.983625\n",
      "Avg validation accuracy 0.921812\n",
      "\n",
      "Optimal p: 1\n",
      "Best validation accuracy: 0.92425\n"
     ]
    }
   ],
   "source": [
    "values = [1, 10, 100, 1000]\n",
    "CV(X_letter, y_letter, 2, 0.2, BAG_DT(i, 0.5, 0.5), values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ANN(hidden_units, i):\n",
    "    clf = MLPClassifier(hidden_units ,alpha = 1)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "p = 0\n",
      "Avg training accuracy: 0.884750\n",
      "Avg validation accuracy 0.878500\n",
      "\n",
      "p = 0.2\n",
      "Avg training accuracy: 0.889125\n",
      "Avg validation accuracy 0.881000\n",
      "\n",
      "p = 0.5\n",
      "Avg training accuracy: 0.896562\n",
      "Avg validation accuracy 0.890313\n",
      "\n",
      "p = 0.9\n",
      "Avg training accuracy: 0.890375\n",
      "Avg validation accuracy 0.886000\n",
      "\n",
      "Optimal p: 0.5\n",
      "Best validation accuracy: 0.8903125000000001\n"
     ]
    }
   ],
   "source": [
    "values = [0,0.2,0.5,0.9]\n",
    "hidden_units = [100]\n",
    "CV(X_letter, y_letter, 2, 0.2, ANN(hidden_units, i), values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition for predict\n",
    "def predict(X_train, y_train, X_test, k):\n",
    "    distances = []\n",
    "    targets = []\n",
    "    label_0 = 0\n",
    "    label_1 = 0\n",
    "    \n",
    "    # Calculate Euclidean distance\n",
    "    for i in range(len(X_train)):\n",
    "        distance = np.sqrt(np.sum(np.square(X_test - X_train[i,:])))\n",
    "        distances.append([distance, i])\n",
    "    \n",
    "    # Sort distances\n",
    "    distances = sorted(distances)\n",
    "    \n",
    "    # Append classes of sorted distances\n",
    "    for i in range(k):\n",
    "        index = distances[i][1]\n",
    "        targets.append(y_train[index])\n",
    "    \n",
    "    # Enumerate classes\n",
    "    for i in range(len(targets)):\n",
    "        if targets[i] == 0:\n",
    "            label_0 += 1\n",
    "        elif targets[i] == 1:\n",
    "            label_1 += 1\n",
    "    \n",
    "    global most_common\n",
    "    # Get most common class\n",
    "    if label_0 > label_1:\n",
    "        most_common = 0\n",
    "    elif label_0 < label_1:\n",
    "        most_common = 1\n",
    "\n",
    "    return most_common\n",
    "\n",
    "# Definititon for kNearestNeighbor\n",
    "def kNearestNeighbor(X_train, y_train, X_test, predictions, k):\n",
    "    for i in range(len(X_test)):\n",
    "        predictions.append(predict(X_train, y_train, X_test[i,:], k))\n",
    "        \n",
    "# Defintion for Cross-validated K-nearest Neighbors\n",
    "def cv_KNN(values_k):\n",
    "    # Cross-validation\n",
    "    # Split into k=5 folds:\n",
    "    n = len(X)\n",
    "    kf = KFold(n_splits=5)\n",
    "    splits = kf.get_n_splits()\n",
    "    count = 0;\n",
    "    avg_acc = np.zeros(splits)\n",
    "    avg_train_acc = np.zeros(splits)\n",
    "\n",
    "    optimal_k = values_k[0]\n",
    "    # Run 5-fold Cross validation\n",
    "    for k in values_k:\n",
    "        fold = 0\n",
    "        score_test = np.zeros(splits)\n",
    "        score_train = np.zeros(splits)\n",
    "        print \"\\nk =\", k\n",
    "    \n",
    "        # Test validation error for each fold\n",
    "        for train_idx, test_idx in kf.split(X_train):\n",
    "            fold += 1\n",
    "            X_train_set, X_test_set = X_train[train_idx], X_train[test_idx]\n",
    "            y_train_set, y_test_set = y_train[train_idx], y_train[test_idx]\n",
    "        \n",
    "            # k nearest neighbors algorithm\n",
    "            predictions = []\n",
    "            kNearestNeighbor(X_train_set, y_train_set, X_test_set, predictions, k)\n",
    "            predictions = np.asarray(predictions)\n",
    "            accuracy = accuracy_score(y_test_set, predictions) * 100\n",
    "        \n",
    "            # Save accuracy to vector\n",
    "            score_test[fold - 1] = accuracy\n",
    "    \n",
    "        avg_acc[count] = np.average(score_test)\n",
    "        print \"Avg validation accuracy: %f\" %(avg_acc[count])\n",
    "        count += 1 \n",
    "    \n",
    "    index_optimal_k = avg_acc.tolist().index(max(avg_acc))\n",
    "    optimal_k = values_k[index_optimal_k]    \n",
    "    \n",
    "    print \"\\nOptimal k:\", optimal_k     \n",
    "    print \"Best validation accuracy:\", np.amax(avg_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-nearest neighbors algorithm is computationally expensive for very large data sets. Shuffle the data to obtain an unbiased sample set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce computation time\n",
    "# Create new variables with shuffled copies of original data\n",
    "X = X_letter\n",
    "y = y_letter\n",
    "X1, y1 = unison_shuffled_copies(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "k = 1\n",
      "Avg validation accuracy: 86.500000\n",
      "\n",
      "k = 2\n",
      "Avg validation accuracy: 84.125000\n",
      "\n",
      "k = 5\n",
      "Avg validation accuracy: 82.875000\n",
      "\n",
      "k = 10\n",
      "Avg validation accuracy: 78.250000\n",
      "\n",
      "Optimal k: 1\n",
      "Best validation accuracy: 86.5\n"
     ]
    }
   ],
   "source": [
    "# 80% train, 20% test\n",
    "# Use a subset of the total data space\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1[:1000], y1[:1000], test_size = 0.2)\n",
    "values_k = [1,2,5,10]\n",
    "cv_KNN(values_k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
